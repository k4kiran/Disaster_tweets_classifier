{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def removeUnicode(text):\n",
    "\t#Removes unicode strings like \"\\u002c\" and \"x96\"\n",
    "\ttext = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n",
    "\ttext = re.sub(r'[^\\x00-\\x7f]',r'',text)\n",
    "\treturn text\n",
    "\n",
    "def replaceURL(text):\n",
    "\t#Replaces url address with \"url\" \n",
    "\ttext = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','<url>',text)\n",
    "\ttext = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\treturn text\n",
    "\n",
    "def replaceAtUser(text):\n",
    "\t#Replaces \"@user\" with \"atUser\"\n",
    "\ttext = re.sub('@[^\\s]+','<user>',text)\n",
    "\treturn text\n",
    "\n",
    "def removeHashtag(text):\n",
    "\t#Removes hastag in front of a word\n",
    "\ttext = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\treturn text\n",
    "\n",
    "def removeNumbers(text):\n",
    "\t#Removes integers\n",
    "\ttext = ''.join([i for i in text if not i.isdigit()])         \n",
    "\treturn text\n",
    "\n",
    "def replaceMulExcl(text):\n",
    "\t#Replaces repetitions of exlamation marks\n",
    "\ttext = re.sub(r\"(\\!)\\1+\", '!', text)\n",
    "\treturn text\n",
    "\n",
    "def replaceMulQues(text):\n",
    "\t#Replaces repetitions of question marks\n",
    "\ttext = re.sub(r\"(\\?)\\1+\", '?', text)\n",
    "\treturn text\n",
    "\n",
    "def replaceMulStop(text):\n",
    "\t#Replaces repetitions of stop marks\n",
    "\ttext = re.sub(r\"(\\.)\\1+\", '.', text)\n",
    "\treturn text\n",
    "\n",
    "def countMulExcl(text):\n",
    "\t#count repetitions of exlamation marks\n",
    "\treturn len(re.findall(r\"(\\!)\\1+\", text))\n",
    "\n",
    "def countMulQues(text):\n",
    "\t#Count repetitions of question marks\n",
    "\treturn len(re.findall(r\"(\\?)\\1+\", text))\n",
    "\n",
    "def countMulStop(text):\n",
    "\t#Count repetitions of stop marks\n",
    "\treturn len(re.findall(r\"(\\.)\\1+\", text))\n",
    "\n",
    "def countElongated(text):\n",
    "\t#count of how many words are elongated\n",
    "\tregex = re.compile(r\"(.)\\1{2}\")\n",
    "\treturn len([word for word in text.split() if regex.search(word)])\n",
    "\n",
    "def countAllCaps(text):\n",
    "\t#count of how many words are all caps\n",
    "\treturn len(re.findall(\"[A-Z0-9]{3,}\", text))\n",
    "\n",
    "#Creates a dictionary with slangs and their equivalents and replaces them\n",
    "with open('slang.txt') as file:\n",
    "\tslang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "\tfor line in file if line.strip())\n",
    "\n",
    "slang_words = sorted(slang_map, key=len, reverse=True)\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
    "\n",
    "#punctuation list for replacing\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '|', ';', \"'\", '$', '&', '/', '[', ']', '%', '=', '*', '+', '\\\\', '•',  '~', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def removePuncts(x):\n",
    "\tx = str(x)\n",
    "\tfor punct in puncts:\n",
    "\t\tif punct in x:\n",
    "\t\t\tx = x.replace(punct, f' ')\n",
    "\treturn x\n",
    "\n",
    "\n",
    "\n",
    "def countSlang(text):\n",
    "\t# counts how many slang words and a list of found slangs\n",
    "\tslangCounter = 0\n",
    "\tslangsFound = []\n",
    "\ttokens = nltk.word_tokenize(text)\n",
    "\tfor word in tokens:\n",
    "\t\tif word in slang_words:\n",
    "\t\t\tslangsFound.append(word)\n",
    "\t\t\tslangCounter += 1\n",
    "\treturn slangCounter, slangsFound\n",
    "\n",
    "#Replaces contractions from a string to their equivalents\n",
    "contraction_patterns = [ (r'I\\'m', 'I am'),(r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "\t\t\t\t\t\t (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "def replaceContraction(text):\n",
    "\tpatterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "\tfor (pattern, repl) in patterns:\n",
    "\t\t(text, count) = re.subn(pattern, repl, text)\n",
    "\treturn text\n",
    "\n",
    "def replaceElongated(word):\n",
    "\t#Replaces an elongated word with its basic form\n",
    "\n",
    "\trepeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "\trepl = r'\\1\\2\\3'\n",
    "\tif wordnet.synsets(word):\n",
    "\t\treturn word\n",
    "\trepl_word = repeat_regexp.sub(repl, word)\n",
    "\tif repl_word != word:      \n",
    "\t\treturn replaceElongated(repl_word)\n",
    "\telse:       \n",
    "\t\treturn repl_word\n",
    "\n",
    "def removeEmoticons(text):\n",
    "\t#Removes emoticons from text \n",
    "\ttext = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n",
    "\treturn text\n",
    "\n",
    "def countEmoticons(text):\n",
    "\t#Input: a text, Output: how many emoticons\n",
    "\treturn len(re.findall(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', text))\n",
    "\n",
    "\n",
    "### Spell Correction begin ###\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('spell_correction.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "\t#P robability of `word`.\n",
    "\treturn WORDS[word] / N\n",
    "\n",
    "def spellCorrection(word): \n",
    "\t#Most probable spelling correction for word.\n",
    "\treturn max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "\t#Generate possible spelling corrections for word.\n",
    "\treturn (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "\t#The subset of `words` that appear in the dictionary of WORDS.\n",
    "\treturn set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "\t#All edits that are one edit away from `word`.\n",
    "\tletters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\tsplits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "\tdeletes    = [L + R[1:]               for L, R in splits if R]\n",
    "\ttransposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "\treplaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "\tinserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "\treturn set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "\t#All edits that are two edits away from `word`.\n",
    "\treturn (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "### Spell Correction End ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from time import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "def k_prep(inputText = 'null'):\n",
    "\t'''\n",
    "\tc = input(\"\\n1.Enter or 2.default or 3.df ? :\")\n",
    "\n",
    "\tif c == \"2\":\n",
    "\t\ttext = \"AFRICA,#AFRICANBAZE: asap goaaaaal @Breaking !!!! news:Nigeria oooooooh :-D aren't flag???? wont set ablaze..... 12000 in America. http://t.co/2nndBGwyEi,1\"\n",
    "\t\t#print(text)\n",
    "\t\n",
    "\telif c == \"3\" :\n",
    "\t\ttext = inputText\n",
    "\n",
    "\telse:\n",
    "\t\ttext = input(\"\\nEnter the tweet: \")\n",
    "\t'''\n",
    "\n",
    "\ttext = inputText\n",
    "\t#print(\"\\nReplacing url from tweet\\n\")\n",
    "\ttext = replaceURL(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nReplacing atuser from tweet\\n\")\n",
    "\ttext = replaceAtUser(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nremoving hashtag in tweet\\n\")\n",
    "\ttext = removeHashtag(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nreplace at user in tweet\\n\")\n",
    "\ttext = replaceAtUser(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t'''\n",
    "\t#print(\"\\nremoving stopwords\\n\")\n",
    "\t#nltk.download('stopwords')\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tstop = set(stopwords.words('english'))\n",
    "\n",
    "\td=[]\n",
    "\td.append([x for x in text.split() if x not in stop])\n",
    "\td = d[0]\n",
    "\ttext = ' '.join(d)\n",
    "\t#print(text)\n",
    "\t'''\n",
    "\n",
    "\t#print(\"\\nremove numbers from tweet\\n\")\n",
    "\ttext = removeNumbers(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nremove emoticons from tweet\\n\")\n",
    "\ttext = removeEmoticons(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#couting multple punctuations\n",
    "\t#print(\"\\ncounting multiple punctuations\\n\")\n",
    "\tMultiExclMarks = 0\n",
    "\tMultiQuesMarks = 0\n",
    "\tMultiStopMarks = 0\n",
    "\n",
    "\tMultiExclMarks += countMulExcl(text)\n",
    "\tMultiQuesMarks += countMulQues(text)\n",
    "\tMultiStopMarks += countMulStop(text)\n",
    "\n",
    "\t#print(MultiExclMarks,MultiQuesMarks,MultiStopMarks)\n",
    "\n",
    "\t#print(\"\\nremove multiexclamations from tweet\\n\")\n",
    "\ttext = replaceMulExcl(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nremove multiquestionmarks from tweet\\n\")\n",
    "\ttext = replaceMulQues(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nremove multistopmarks from tweet\\n\")\n",
    "\ttext = replaceMulStop(text)\n",
    "\t#print(text)\n",
    "\n",
    "\n",
    "\t#print(\"\\nshortening elongated words\\n\")\n",
    "\ttotalElongated = 0\n",
    "\ttotalElongated += countElongated(text)\n",
    "\t#print(totalElongated)\n",
    "\n",
    "\tregex1 = re.compile(r\"(.)\\1{2}\")\n",
    "\tl=[]\n",
    "\tfor word in text.split():\n",
    "\t\tif(regex1.search(word)):\n",
    "\t\t\tnew_word = replaceElongated(word)\n",
    "\t\t\t##print(new_word)\n",
    "\t\t\tl.append(new_word)\n",
    "\t\telse:\n",
    "\t\t\tl.append(word)\n",
    "\ttext = ' '.join(l)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nRemoving punctuations except ?!\\n\")\n",
    "\ttext = removePuncts(text)\n",
    "\t#print(text)\n",
    "\t\n",
    "\t#print(\"\\nexpanding slangs in tweet\\n\")\n",
    "\ttext = replaceSlang(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nreplace contractions in tweet\\n\")\n",
    "\ttext = replaceContraction(text)\n",
    "\t#print(text)\n",
    "\t\n",
    "\t#print(\\nTokenizing the text\\n\")\n",
    "\ttext = word_tokenize(text)\n",
    "\t\n",
    "\t#print(\"\\nLemmatizing the text\\n\")\n",
    "\tlemma = WordNetLemmatizer()\n",
    "\t\n",
    "\tlist1 = []\n",
    "\tfor txt in text:\n",
    "\t\tlist1.append(lemma.lemmatize(txt))\n",
    "\t\t\n",
    "\treturn list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Our, Deeds, are, the, Reason, of, this, earth...\n",
      "1           [Forest, fire, near, La, Ronge, Sask, Canada]\n",
      "2       [All, resident, asked, to, shelter, in, place,...\n",
      "3       [people, receive, wildfire, evacuation, order,...\n",
      "4       [Just, got, sent, this, photo, from, Ruby, Ala...\n",
      "                              ...                        \n",
      "7608    [Two, giant, crane, holding, a, bridge, collap...\n",
      "7609    [<, user, >, <, user, >, The, out, of, control...\n",
      "7610    [M, UTC, ?, km, S, of, Volcano, Hawaii, <, url...\n",
      "7611    [Police, investigating, after, an, e, bike, co...\n",
      "7612    [The, Latest, More, Homes, Razed, by, Northern...\n",
      "Name: text, Length: 7613, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "df = pd.read_csv(\"../dataset/train.csv\")\n",
    "df['text'] = df['text'].apply(k_prep)\n",
    "print(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.63671875e-02,  6.83593750e-02, -3.11279297e-02,  2.19726562e-01,\n",
       "        3.41796875e-03, -9.03320312e-03,  1.07910156e-01, -1.74804688e-01,\n",
       "        7.71484375e-02,  3.83377075e-04, -1.02539062e-01, -1.73339844e-02,\n",
       "       -3.08837891e-02,  5.76171875e-02, -1.09863281e-01,  6.10351562e-02,\n",
       "        2.48046875e-01,  5.46264648e-03,  3.49121094e-02,  7.65991211e-03,\n",
       "       -1.07910156e-01,  2.16796875e-01,  1.26953125e-01,  1.46484375e-01,\n",
       "        1.55273438e-01,  4.46777344e-02,  7.51953125e-02, -1.45507812e-01,\n",
       "       -7.71484375e-02, -8.54492188e-02, -1.15966797e-02,  7.95898438e-02,\n",
       "       -1.94335938e-01, -2.57812500e-01, -9.86328125e-02, -1.38671875e-01,\n",
       "       -4.13894653e-04, -9.03320312e-02,  7.66601562e-02,  1.33789062e-01,\n",
       "        5.17578125e-02, -4.80957031e-02,  1.88476562e-01, -3.44238281e-02,\n",
       "       -7.95898438e-02,  3.54003906e-02, -1.02539062e-01, -1.33789062e-01,\n",
       "       -1.06445312e-01,  7.42187500e-02, -2.46582031e-02,  1.99218750e-01,\n",
       "        1.54296875e-01, -1.22558594e-01,  4.71191406e-02, -3.00781250e-01,\n",
       "       -7.37304688e-02, -6.44531250e-02,  9.08203125e-02,  6.68945312e-02,\n",
       "       -3.68652344e-02, -1.48437500e-01, -1.24511719e-02,  7.12890625e-02,\n",
       "       -6.34765625e-02, -2.05078125e-01, -2.39257812e-01,  9.17968750e-02,\n",
       "       -3.56445312e-02,  6.54296875e-02,  7.47070312e-02,  1.34765625e-01,\n",
       "        1.41601562e-01, -4.54101562e-02, -1.91406250e-01, -2.61718750e-01,\n",
       "        2.01416016e-02,  1.17187500e-01,  3.51562500e-02,  8.00781250e-02,\n",
       "       -1.98364258e-03, -9.17968750e-02,  1.27929688e-01,  2.95410156e-02,\n",
       "        9.66796875e-02,  8.88671875e-02,  5.66406250e-02,  6.54296875e-02,\n",
       "       -2.92968750e-02,  2.19726562e-02,  1.22558594e-01,  3.10546875e-01,\n",
       "        1.36108398e-02, -1.68945312e-01, -3.97949219e-02, -9.66796875e-02,\n",
       "        1.96289062e-01,  2.63671875e-01, -2.13623047e-02,  2.67028809e-03,\n",
       "       -1.45507812e-01,  7.42187500e-02,  2.14843750e-02,  5.85937500e-02,\n",
       "       -5.02929688e-02, -1.98242188e-01,  5.29785156e-02, -1.74804688e-01,\n",
       "        1.02050781e-01, -1.23901367e-02,  1.27929688e-01, -1.02539062e-01,\n",
       "       -1.25000000e-01, -1.79687500e-01, -3.10058594e-02,  4.00390625e-02,\n",
       "        7.86132812e-02, -1.93359375e-01,  1.42578125e-01,  3.39355469e-02,\n",
       "        9.82666016e-03,  4.68750000e-02, -2.91442871e-03,  2.75390625e-01,\n",
       "       -7.50732422e-03,  1.63085938e-01, -4.88281250e-02, -7.47070312e-02,\n",
       "       -1.27929688e-01, -5.20019531e-02, -1.53320312e-01, -1.42578125e-01,\n",
       "        1.40625000e-01,  1.86523438e-01,  2.52685547e-02, -1.07910156e-01,\n",
       "        9.37500000e-02,  2.10937500e-01,  1.16699219e-01,  1.02050781e-01,\n",
       "        1.16699219e-01, -7.91015625e-02,  4.34570312e-02,  1.17187500e-01,\n",
       "        1.24511719e-01,  9.81445312e-02, -2.47070312e-01, -2.35351562e-01,\n",
       "       -1.03027344e-01, -1.12304688e-01, -6.98242188e-02,  1.26953125e-01,\n",
       "       -2.42919922e-02,  5.51757812e-02, -1.09375000e-01, -1.03027344e-01,\n",
       "        9.52148438e-02, -1.95312500e-01, -1.52343750e-01,  1.48437500e-01,\n",
       "        8.83789062e-02,  4.83398438e-02,  1.37329102e-02, -6.12792969e-02,\n",
       "        5.00488281e-02, -3.10546875e-01,  9.32617188e-02,  1.21459961e-02,\n",
       "        9.57031250e-02, -1.62109375e-01, -2.33398438e-01, -2.61718750e-01,\n",
       "       -5.12695312e-02, -7.42187500e-02, -4.90722656e-02,  1.06445312e-01,\n",
       "        1.13281250e-01, -2.97851562e-02,  5.46875000e-02,  1.60156250e-01,\n",
       "       -2.12890625e-01, -1.42578125e-01, -1.66015625e-02,  1.54296875e-01,\n",
       "        1.41601562e-02, -9.91210938e-02,  8.44726562e-02,  8.74023438e-02,\n",
       "        2.00195312e-01, -5.07812500e-02,  2.07031250e-01,  1.27929688e-01,\n",
       "        1.75781250e-01,  1.78710938e-01, -1.58691406e-02,  4.05883789e-03,\n",
       "       -5.40161133e-03, -8.48388672e-03, -1.23596191e-03, -1.42578125e-01,\n",
       "       -9.81445312e-02,  3.90625000e-02,  1.22680664e-02,  6.33239746e-04,\n",
       "        5.27343750e-02,  9.37500000e-02, -3.22265625e-02, -2.89062500e-01,\n",
       "        1.27929688e-01, -5.98144531e-03,  7.61718750e-02,  7.17773438e-02,\n",
       "        8.34960938e-02,  2.07519531e-02, -1.96289062e-01,  6.78710938e-02,\n",
       "        2.08007812e-01, -1.08642578e-02, -4.44335938e-02, -2.77343750e-01,\n",
       "       -1.17675781e-01, -1.33789062e-01,  8.15429688e-02,  1.25122070e-02,\n",
       "        1.81640625e-01, -7.51953125e-02,  1.12792969e-01,  7.56835938e-02,\n",
       "        5.05371094e-02, -5.85937500e-02,  5.29785156e-02, -4.37011719e-02,\n",
       "       -5.88378906e-02,  1.54296875e-01,  9.46044922e-03,  1.16210938e-01,\n",
       "       -1.51367188e-01,  1.11083984e-02,  1.41601562e-01, -5.54199219e-02,\n",
       "        1.83593750e-01,  1.17187500e-01, -1.27929688e-01, -7.08007812e-02,\n",
       "        3.72314453e-03, -4.19921875e-02, -9.13085938e-02,  1.09375000e-01,\n",
       "        1.27929688e-01,  2.25830078e-02,  1.93359375e-01,  5.46875000e-02,\n",
       "        2.25830078e-02, -3.22265625e-02,  1.25000000e-01, -5.00488281e-02,\n",
       "       -3.34472656e-02, -6.16455078e-03, -3.27148438e-02, -3.06396484e-02,\n",
       "       -1.31835938e-01, -6.73828125e-02, -1.04980469e-01,  1.26953125e-01,\n",
       "        1.79687500e-01,  7.56835938e-02,  1.08398438e-01,  8.93554688e-02,\n",
       "       -1.72119141e-02,  9.08203125e-02,  6.65283203e-03,  2.30468750e-01,\n",
       "        3.32031250e-02,  1.69921875e-01,  1.49414062e-01, -2.08007812e-01,\n",
       "       -5.98144531e-02, -1.65039062e-01, -9.17968750e-02, -1.46865845e-04,\n",
       "        8.20312500e-02, -1.25000000e-01,  9.88769531e-03,  1.07421875e-01,\n",
       "       -1.04370117e-02, -1.96533203e-02, -2.30712891e-02,  8.88671875e-02,\n",
       "       -1.48925781e-02,  2.18750000e-01,  4.12597656e-02,  2.98828125e-01,\n",
       "       -2.32421875e-01,  1.65039062e-01, -2.23388672e-02, -6.44531250e-02,\n",
       "        1.59179688e-01, -1.70898438e-01,  1.96289062e-01, -9.96093750e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the w2c model\n",
    "# use this or below one\n",
    "w2c = gensim.models.KeyedVectors.load_word2vec_format('../../../temp/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "w2c['go']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_number(num):\n",
    "    mylist = []\n",
    "    for i in range(1,num):\n",
    "        mylist.append(str(i))\n",
    "    return mylist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list(n):\n",
    "    import random\n",
    "    randomlist = []\n",
    "    for i in range(0,n):\n",
    "        n = random.uniform(-2,2)\n",
    "        randomlist.append(n)\n",
    "    return randomlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using glove instead fo w2c\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "#glove_file = datapath('../../../temp/glove/glove.twitter.27B.25d.txt')\n",
    "#tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "#glove2word2vec(glove_file, tmp_file)\n",
    "glove2word2vec(glove_input_file=\"../../../temp/glove/glove.twitter.27B.200d.txt\", word2vec_output_file=\"gensim_glove_vectors.txt\")\n",
    "w2c = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\")\n",
    "w2c['go']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Returns mean of w2v value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def document_vector(model,doc):\n",
    "\t#filename = 'glove.twitter.27B.25d.txt.word2vec'\n",
    "\t#model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\t# remove out-of-vocabulary words\n",
    "\tdoc = [word for word in doc if word in model.vocab]\n",
    "\treturn np.mean(model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f5667247053b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'document_vector' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f5667247053b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;36m0.38142973\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             ]\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m#missed.append(str(index) + ' ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m#print(e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_list' is not defined"
     ]
    }
   ],
   "source": [
    "new_df = pd.DataFrame()\n",
    "final_df = []\n",
    "missed = []\n",
    "l = []\n",
    "for index, row in df.iterrows():\n",
    "    string1 = row['text']\n",
    "    #l.append(string1)\n",
    "    try:\n",
    "        result = document_vector(w2c, string1)\n",
    "        #print(result)\n",
    "        l.append(list(result))\n",
    "        #print(l)\n",
    "        #final_df.append(l)\n",
    "        #a_series = pd.Series(l)\n",
    "        #print(l)\n",
    "\n",
    "        #new_df = new_df.append(a_series,ignore_index=True)\n",
    "    except Exception as e:\n",
    "        list1 = [\n",
    "            0.69671315,\n",
    "            0.049782764,\n",
    "            -0.24523668,\n",
    "            -0.15872465,\n",
    "            -0.0665417,\n",
    "            0.20241983,\n",
    "            0.077576466,\n",
    "            1.9189811,\n",
    "            -0.006817713,\n",
    "            0.06951927,\n",
    "            -0.4152605,\n",
    "            -0.89838743,\n",
    "            -3.7327635,\n",
    "            -0.049629666,\n",
    "            -0.7617386,\n",
    "            -0.5561758,\n",
    "            -0.9451503,\n",
    "            0.035365578,\n",
    "            -1.0393411,\n",
    "            0.2922259,\n",
    "            -0.16664228,\n",
    "            -0.46666014,\n",
    "            0.35039356,\n",
    "            0.40681368,\n",
    "            0.38142973,\n",
    "            ]\n",
    "        l.append(create_list(300))\n",
    "        #missed.append(str(index) + ' ')\n",
    "        #print(e)\n",
    "        #final_df.append(l)\n",
    "        #a_series = pd.Series(l)\n",
    "        #new_df.append(a_series,ignore_index=True)\n",
    "print(l[1])\n",
    "len(l[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "7608    1\n",
      "7609    1\n",
      "7610    1\n",
      "7611    1\n",
      "7612    1\n",
      "Name: target, Length: 7613, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create the pandas DataFrame \n",
    "label_df = df['target']\n",
    "result_df = pd.DataFrame(l, columns = create_number(301))\n",
    "#print(result_df)\n",
    "print(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             1         2         3         4         5         6         7  \\\n",
      "0     0.021657  0.078870  0.076080  0.082520 -0.118947  0.011986 -0.007594   \n",
      "1     0.077427  0.018101  0.116699  0.069196  0.036717 -0.041347 -0.107396   \n",
      "2    -0.014340 -0.006574  0.084782 -0.012732 -0.063611 -0.023063  0.054099   \n",
      "3     0.177699 -0.047241  0.004203  0.100237  0.032628 -0.004854 -0.087019   \n",
      "4     0.086696  0.004797 -0.040091  0.059727 -0.036028 -0.023974 -0.019771   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "7608 -0.044428 -0.030121 -0.004439  0.026434  0.019830 -0.139926  0.021290   \n",
      "7609  0.000160  0.034534  0.028207  0.069690 -0.031021 -0.015558 -0.004175   \n",
      "7610 -0.170639 -0.028827  0.004154  0.250061  0.020477 -0.069992 -0.123230   \n",
      "7611  0.041909  0.047636  0.053714 -0.044213 -0.061146  0.001296  0.023490   \n",
      "7612  0.012226 -0.009672 -0.006827 -0.012755  0.018559  0.037983  0.033324   \n",
      "\n",
      "             8         9        10  ...       291       292       293  \\\n",
      "0    -0.057470  0.057719  0.162974  ... -0.053233  0.029869 -0.097677   \n",
      "1    -0.026311 -0.022112 -0.023856  ...  0.011632  0.005319 -0.139248   \n",
      "2    -0.033514  0.086780  0.029407  ... -0.112252 -0.048998 -0.103353   \n",
      "3    -0.037641 -0.013938 -0.041678  ...  0.113613 -0.097866 -0.112130   \n",
      "4    -0.103149 -0.033643  0.091697  ... -0.007926 -0.081602 -0.078688   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "7608 -0.063937  0.133634  0.035972  ... -0.077381  0.015780 -0.085127   \n",
      "7609 -0.020152  0.048962  0.042326  ...  0.030541  0.002739 -0.046342   \n",
      "7610 -0.113190 -0.014473  0.001022  ...  0.097565  0.090401 -0.133423   \n",
      "7611 -0.106899  0.031756  0.046156  ...  0.013393 -0.032360 -0.066005   \n",
      "7612  0.027944  0.062068  0.102943  ...  0.096168 -0.080872 -0.111140   \n",
      "\n",
      "           294       295       296       297       298       299       300  \n",
      "0    -0.064779 -0.019053 -0.084010 -0.077952 -0.070905  0.001058  0.085795  \n",
      "1     0.151437 -0.059169 -0.025251 -0.089443 -0.035697  0.063320 -0.025229  \n",
      "2     0.017770  0.052502  0.049498 -0.067738  0.011725  0.011369 -0.015718  \n",
      "3     0.035366  0.191023  0.057757 -0.177176  0.074951  0.003723 -0.002886  \n",
      "4     0.032741 -0.001305 -0.013593  0.074550 -0.037988  0.007891  0.021899  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "7608 -0.062844  0.028121 -0.058128  0.031583 -0.080422 -0.024938 -0.070945  \n",
      "7609  0.047308  0.022089 -0.043234 -0.002067  0.019519  0.043472 -0.029207  \n",
      "7610 -0.034088  0.096466 -0.108032 -0.091553  0.026245  0.004948  0.038177  \n",
      "7611  0.030371 -0.038423 -0.023776  0.006865 -0.020031 -0.009076 -0.031666  \n",
      "7612  0.058035  0.103722 -0.146400 -0.074316  0.000329  0.026358 -0.021269  \n",
      "\n",
      "[7613 rows x 300 columns]\n"
     ]
    }
   ],
   "source": [
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df,label_df, test_size=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring variables\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 30 \n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd method\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kiranbal/anaconda3/envs/mlvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/kiranbal/anaconda3/envs/mlvenv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#Creating embedding layer for testing neural network\n",
    "import tensorflow as tf\n",
    "vocab_size = 270000000000000\n",
    "embedding_dim = 300\n",
    "max_length = 300\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "     tf.keras.layers.Flatten(),\n",
    "     tf.keras.layers.Dense(6, activation='relu'),\n",
    "     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 300)          8100000000\n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 90000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 540006    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 81,000,000,000,540,013\n",
      "Trainable params: 81,000,000,000,540,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kiranbal/anaconda3/envs/mlvenv/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#adding optimizer and loss functions for the model\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c9caa69e5df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "     tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "     tf.keras.layers.GlobalAveragePooling1D(),\n",
    "     tf.keras.layers.Dense(6, activation='relu'),\n",
    "     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 300, 16)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 296, 128)          10368     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,149\n",
      "Trainable params: 171,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding optimizer and loss functions for the model\n",
    "model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4948 samples\n",
      "Epoch 1/10\n",
      "4948/4948 [==============================] - 0s 82us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 2/10\n",
      "4948/4948 [==============================] - 0s 93us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.4958\n",
      "Epoch 3/10\n",
      "4948/4948 [==============================] - 0s 96us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 4/10\n",
      "4948/4948 [==============================] - 0s 96us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 5/10\n",
      "4948/4948 [==============================] - 0s 100us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 6/10\n",
      "4948/4948 [==============================] - 1s 112us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.4972\n",
      "Epoch 7/10\n",
      "4948/4948 [==============================] - 1s 104us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 8/10\n",
      "4948/4948 [==============================] - 1s 128us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 9/10\n",
      "4948/4948 [==============================] - 1s 128us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 10/10\n",
      "4948/4948 [==============================] - 1s 105us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kiranbal/anaconda3/envs/mlvenv/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/kiranbal/anaconda3/envs/mlvenv/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/kiranbal/anaconda3/envs/mlvenv/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model3 = tf.keras.Sequential([\n",
    "     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "     tf.keras.layers.Dense(64, activation='relu'),\n",
    "     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 300, 16)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 296, 128)          10368     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,149\n",
      "Trainable params: 171,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding optimizer and loss functions for the model\n",
    "model3.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4948 samples\n",
      "Epoch 1/10\n",
      "4948/4948 [==============================] - 0s 86us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 2/10\n",
      "4948/4948 [==============================] - 0s 81us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 3/10\n",
      "4948/4948 [==============================] - 0s 80us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 4/10\n",
      "4948/4948 [==============================] - 0s 80us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 5/10\n",
      "4948/4948 [==============================] - 0s 80us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 6/10\n",
      "4948/4948 [==============================] - 0s 81us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.4988\n",
      "Epoch 7/10\n",
      "4948/4948 [==============================] - 0s 99us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 8/10\n",
      "4948/4948 [==============================] - 0s 98us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 9/10\n",
      "4948/4948 [==============================] - 0s 86us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.5000\n",
      "Epoch 10/10\n",
      "4948/4948 [==============================] - 0s 84us/sample - loss: 0.6820 - acc: 0.5746 - auc_2: 0.4891\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "mlvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
