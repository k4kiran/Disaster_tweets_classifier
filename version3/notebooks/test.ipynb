{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "def removeUnicode(text):\n",
    "\t#Removes unicode strings like \"\\u002c\" and \"x96\"\n",
    "\ttext = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n",
    "\ttext = re.sub(r'[^\\x00-\\x7f]',r'',text)\n",
    "\treturn text\n",
    "\n",
    "def replaceURL(text):\n",
    "\t#Replaces url address with \"url\" \n",
    "\ttext = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','<url>',text)\n",
    "\ttext = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\treturn text\n",
    "\n",
    "def replaceAtUser(text):\n",
    "\t#Replaces \"@user\" with \"atUser\"\n",
    "\ttext = re.sub('@[^\\s]+','<user>',text)\n",
    "\treturn text\n",
    "\n",
    "def removeHashtag(text):\n",
    "\t#Removes hastag in front of a word\n",
    "\ttext = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\treturn text\n",
    "\n",
    "def removeNumbers(text):\n",
    "\t#Removes integers\n",
    "\ttext = ''.join([i for i in text if not i.isdigit()])         \n",
    "\treturn text\n",
    "\n",
    "def replaceMulExcl(text):\n",
    "\t#Replaces repetitions of exlamation marks\n",
    "\ttext = re.sub(r\"(\\!)\\1+\", '!', text)\n",
    "\treturn text\n",
    "\n",
    "def replaceMulQues(text):\n",
    "\t#Replaces repetitions of question marks\n",
    "\ttext = re.sub(r\"(\\?)\\1+\", '?', text)\n",
    "\treturn text\n",
    "\n",
    "def replaceMulStop(text):\n",
    "\t#Replaces repetitions of stop marks\n",
    "\ttext = re.sub(r\"(\\.)\\1+\", '.', text)\n",
    "\treturn text\n",
    "\n",
    "def countMulExcl(text):\n",
    "\t#count repetitions of exlamation marks\n",
    "\treturn len(re.findall(r\"(\\!)\\1+\", text))\n",
    "\n",
    "def countMulQues(text):\n",
    "\t#Count repetitions of question marks\n",
    "\treturn len(re.findall(r\"(\\?)\\1+\", text))\n",
    "\n",
    "def countMulStop(text):\n",
    "\t#Count repetitions of stop marks\n",
    "\treturn len(re.findall(r\"(\\.)\\1+\", text))\n",
    "\n",
    "def countElongated(text):\n",
    "\t#count of how many words are elongated\n",
    "\tregex = re.compile(r\"(.)\\1{2}\")\n",
    "\treturn len([word for word in text.split() if regex.search(word)])\n",
    "\n",
    "def countAllCaps(text):\n",
    "\t#count of how many words are all caps\n",
    "\treturn len(re.findall(\"[A-Z0-9]{3,}\", text))\n",
    "\n",
    "#Creates a dictionary with slangs and their equivalents and replaces them\n",
    "with open('slang.txt') as file:\n",
    "\tslang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "\tfor line in file if line.strip())\n",
    "\n",
    "slang_words = sorted(slang_map, key=len, reverse=True)\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
    "\n",
    "#punctuation list for replacing\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '|', ';', \"'\", '$', '&', '/', '[', ']', '%', '=', '*', '+', '\\\\', '•',  '~', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def removePuncts(x):\n",
    "\tx = str(x)\n",
    "\tfor punct in puncts:\n",
    "\t\tif punct in x:\n",
    "\t\t\tx = x.replace(punct, f' ')\n",
    "\treturn x\n",
    "\n",
    "\n",
    "\n",
    "def countSlang(text):\n",
    "\t# counts how many slang words and a list of found slangs\n",
    "\tslangCounter = 0\n",
    "\tslangsFound = []\n",
    "\ttokens = nltk.word_tokenize(text)\n",
    "\tfor word in tokens:\n",
    "\t\tif word in slang_words:\n",
    "\t\t\tslangsFound.append(word)\n",
    "\t\t\tslangCounter += 1\n",
    "\treturn slangCounter, slangsFound\n",
    "\n",
    "#Replaces contractions from a string to their equivalents\n",
    "contraction_patterns = [ (r'I\\'m', 'I am'),(r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "\t\t\t\t\t\t (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "def replaceContraction(text):\n",
    "\tpatterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "\tfor (pattern, repl) in patterns:\n",
    "\t\t(text, count) = re.subn(pattern, repl, text)\n",
    "\treturn text\n",
    "\n",
    "def replaceElongated(word):\n",
    "\t#Replaces an elongated word with its basic form\n",
    "\n",
    "\trepeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "\trepl = r'\\1\\2\\3'\n",
    "\tif wordnet.synsets(word):\n",
    "\t\treturn word\n",
    "\trepl_word = repeat_regexp.sub(repl, word)\n",
    "\tif repl_word != word:      \n",
    "\t\treturn replaceElongated(repl_word)\n",
    "\telse:       \n",
    "\t\treturn repl_word\n",
    "\n",
    "def removeEmoticons(text):\n",
    "\t#Removes emoticons from text \n",
    "\ttext = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n",
    "\treturn text\n",
    "\n",
    "def countEmoticons(text):\n",
    "\t#Input: a text, Output: how many emoticons\n",
    "\treturn len(re.findall(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', text))\n",
    "\n",
    "\n",
    "### Spell Correction begin ###\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('spell_correction.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "\t#P robability of `word`.\n",
    "\treturn WORDS[word] / N\n",
    "\n",
    "def spellCorrection(word): \n",
    "\t#Most probable spelling correction for word.\n",
    "\treturn max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "\t#Generate possible spelling corrections for word.\n",
    "\treturn (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "\t#The subset of `words` that appear in the dictionary of WORDS.\n",
    "\treturn set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "\t#All edits that are one edit away from `word`.\n",
    "\tletters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\tsplits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "\tdeletes    = [L + R[1:]               for L, R in splits if R]\n",
    "\ttransposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "\treplaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "\tinserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "\treturn set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "\t#All edits that are two edits away from `word`.\n",
    "\treturn (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "### Spell Correction End ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from time import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "def k_prep(inputText = 'null'):\n",
    "\t'''\n",
    "\tc = input(\"\\n1.Enter or 2.default or 3.df ? :\")\n",
    "\n",
    "\tif c == \"2\":\n",
    "\t\ttext = \"AFRICA,#AFRICANBAZE: asap goaaaaal @Breaking !!!! news:Nigeria oooooooh :-D aren't flag???? wont set ablaze..... 12000 in America. http://t.co/2nndBGwyEi,1\"\n",
    "\t\t#print(text)\n",
    "\t\n",
    "\telif c == \"3\" :\n",
    "\t\ttext = inputText\n",
    "\n",
    "\telse:\n",
    "\t\ttext = input(\"\\nEnter the tweet: \")\n",
    "\t'''\n",
    "\n",
    "\ttext = inputText\n",
    "\t#print(\"\\nReplacing url from tweet\\n\")\n",
    "\ttext = replaceURL(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nReplacing atuser from tweet\\n\")\n",
    "\ttext = replaceAtUser(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nremoving hashtag in tweet\\n\")\n",
    "\ttext = removeHashtag(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nreplace at user in tweet\\n\")\n",
    "\ttext = replaceAtUser(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t'''\n",
    "\t#print(\"\\nremoving stopwords\\n\")\n",
    "\t#nltk.download('stopwords')\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tstop = set(stopwords.words('english'))\n",
    "\n",
    "\td=[]\n",
    "\td.append([x for x in text.split() if x not in stop])\n",
    "\td = d[0]\n",
    "\ttext = ' '.join(d)\n",
    "\t#print(text)\n",
    "\t'''\n",
    "\n",
    "\t#print(\"\\nremove numbers from tweet\\n\")\n",
    "\ttext = removeNumbers(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nremove emoticons from tweet\\n\")\n",
    "\ttext = removeEmoticons(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#couting multple punctuations\n",
    "\t#print(\"\\ncounting multiple punctuations\\n\")\n",
    "\tMultiExclMarks = 0\n",
    "\tMultiQuesMarks = 0\n",
    "\tMultiStopMarks = 0\n",
    "\n",
    "\tMultiExclMarks += countMulExcl(text)\n",
    "\tMultiQuesMarks += countMulQues(text)\n",
    "\tMultiStopMarks += countMulStop(text)\n",
    "\n",
    "\t#print(MultiExclMarks,MultiQuesMarks,MultiStopMarks)\n",
    "\n",
    "\t#print(\"\\nremove multiexclamations from tweet\\n\")\n",
    "\ttext = replaceMulExcl(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nremove multiquestionmarks from tweet\\n\")\n",
    "\ttext = replaceMulQues(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nremove multistopmarks from tweet\\n\")\n",
    "\ttext = replaceMulStop(text)\n",
    "\t#print(text)\n",
    "\n",
    "\n",
    "\t#print(\"\\nshortening elongated words\\n\")\n",
    "\ttotalElongated = 0\n",
    "\ttotalElongated += countElongated(text)\n",
    "\t#print(totalElongated)\n",
    "\n",
    "\tregex1 = re.compile(r\"(.)\\1{2}\")\n",
    "\tl=[]\n",
    "\tfor word in text.split():\n",
    "\t\tif(regex1.search(word)):\n",
    "\t\t\tnew_word = replaceElongated(word)\n",
    "\t\t\t##print(new_word)\n",
    "\t\t\tl.append(new_word)\n",
    "\t\telse:\n",
    "\t\t\tl.append(word)\n",
    "\ttext = ' '.join(l)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nRemoving punctuations except ?!\\n\")\n",
    "\ttext = removePuncts(text)\n",
    "\t#print(text)\n",
    "\t\n",
    "\t#print(\"\\nexpanding slangs in tweet\\n\")\n",
    "\ttext = replaceSlang(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nreplace contractions in tweet\\n\")\n",
    "\ttext = replaceContraction(text)\n",
    "\t#print(text)\n",
    "\t\n",
    "\t#print(\\nTokenizing the text\\n\")\n",
    "\ttext = word_tokenize(text)\n",
    "\t\n",
    "\t#print(\"\\nLemmatizing the text\\n\")\n",
    "\tlemma = WordNetLemmatizer()\n",
    "\t\n",
    "\tlist1 = []\n",
    "\tfor txt in text:\n",
    "\t\tlist1.append(lemma.lemmatize(txt))\n",
    "\t\t\n",
    "\treturn list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Our, Deeds, are, the, Reason, of, this, earth...\n",
      "1           [Forest, fire, near, La, Ronge, Sask, Canada]\n",
      "2       [All, resident, asked, to, shelter, in, place,...\n",
      "3       [people, receive, wildfire, evacuation, order,...\n",
      "4       [Just, got, sent, this, photo, from, Ruby, Ala...\n",
      "                              ...                        \n",
      "7608    [Two, giant, crane, holding, a, bridge, collap...\n",
      "7609    [<, user, >, <, user, >, The, out, of, control...\n",
      "7610    [M, UTC, ?, km, S, of, Volcano, Hawaii, <, url...\n",
      "7611    [Police, investigating, after, an, e, bike, co...\n",
      "7612    [The, Latest, More, Homes, Razed, by, Northern...\n",
      "Name: text, Length: 7613, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "df = pd.read_csv(\"../dataset/train.csv\")\n",
    "df['text'] = df['text'].apply(k_prep)\n",
    "print(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.63671875e-02,  6.83593750e-02, -3.11279297e-02,  2.19726562e-01,\n",
       "        3.41796875e-03, -9.03320312e-03,  1.07910156e-01, -1.74804688e-01,\n",
       "        7.71484375e-02,  3.83377075e-04, -1.02539062e-01, -1.73339844e-02,\n",
       "       -3.08837891e-02,  5.76171875e-02, -1.09863281e-01,  6.10351562e-02,\n",
       "        2.48046875e-01,  5.46264648e-03,  3.49121094e-02,  7.65991211e-03,\n",
       "       -1.07910156e-01,  2.16796875e-01,  1.26953125e-01,  1.46484375e-01,\n",
       "        1.55273438e-01,  4.46777344e-02,  7.51953125e-02, -1.45507812e-01,\n",
       "       -7.71484375e-02, -8.54492188e-02, -1.15966797e-02,  7.95898438e-02,\n",
       "       -1.94335938e-01, -2.57812500e-01, -9.86328125e-02, -1.38671875e-01,\n",
       "       -4.13894653e-04, -9.03320312e-02,  7.66601562e-02,  1.33789062e-01,\n",
       "        5.17578125e-02, -4.80957031e-02,  1.88476562e-01, -3.44238281e-02,\n",
       "       -7.95898438e-02,  3.54003906e-02, -1.02539062e-01, -1.33789062e-01,\n",
       "       -1.06445312e-01,  7.42187500e-02, -2.46582031e-02,  1.99218750e-01,\n",
       "        1.54296875e-01, -1.22558594e-01,  4.71191406e-02, -3.00781250e-01,\n",
       "       -7.37304688e-02, -6.44531250e-02,  9.08203125e-02,  6.68945312e-02,\n",
       "       -3.68652344e-02, -1.48437500e-01, -1.24511719e-02,  7.12890625e-02,\n",
       "       -6.34765625e-02, -2.05078125e-01, -2.39257812e-01,  9.17968750e-02,\n",
       "       -3.56445312e-02,  6.54296875e-02,  7.47070312e-02,  1.34765625e-01,\n",
       "        1.41601562e-01, -4.54101562e-02, -1.91406250e-01, -2.61718750e-01,\n",
       "        2.01416016e-02,  1.17187500e-01,  3.51562500e-02,  8.00781250e-02,\n",
       "       -1.98364258e-03, -9.17968750e-02,  1.27929688e-01,  2.95410156e-02,\n",
       "        9.66796875e-02,  8.88671875e-02,  5.66406250e-02,  6.54296875e-02,\n",
       "       -2.92968750e-02,  2.19726562e-02,  1.22558594e-01,  3.10546875e-01,\n",
       "        1.36108398e-02, -1.68945312e-01, -3.97949219e-02, -9.66796875e-02,\n",
       "        1.96289062e-01,  2.63671875e-01, -2.13623047e-02,  2.67028809e-03,\n",
       "       -1.45507812e-01,  7.42187500e-02,  2.14843750e-02,  5.85937500e-02,\n",
       "       -5.02929688e-02, -1.98242188e-01,  5.29785156e-02, -1.74804688e-01,\n",
       "        1.02050781e-01, -1.23901367e-02,  1.27929688e-01, -1.02539062e-01,\n",
       "       -1.25000000e-01, -1.79687500e-01, -3.10058594e-02,  4.00390625e-02,\n",
       "        7.86132812e-02, -1.93359375e-01,  1.42578125e-01,  3.39355469e-02,\n",
       "        9.82666016e-03,  4.68750000e-02, -2.91442871e-03,  2.75390625e-01,\n",
       "       -7.50732422e-03,  1.63085938e-01, -4.88281250e-02, -7.47070312e-02,\n",
       "       -1.27929688e-01, -5.20019531e-02, -1.53320312e-01, -1.42578125e-01,\n",
       "        1.40625000e-01,  1.86523438e-01,  2.52685547e-02, -1.07910156e-01,\n",
       "        9.37500000e-02,  2.10937500e-01,  1.16699219e-01,  1.02050781e-01,\n",
       "        1.16699219e-01, -7.91015625e-02,  4.34570312e-02,  1.17187500e-01,\n",
       "        1.24511719e-01,  9.81445312e-02, -2.47070312e-01, -2.35351562e-01,\n",
       "       -1.03027344e-01, -1.12304688e-01, -6.98242188e-02,  1.26953125e-01,\n",
       "       -2.42919922e-02,  5.51757812e-02, -1.09375000e-01, -1.03027344e-01,\n",
       "        9.52148438e-02, -1.95312500e-01, -1.52343750e-01,  1.48437500e-01,\n",
       "        8.83789062e-02,  4.83398438e-02,  1.37329102e-02, -6.12792969e-02,\n",
       "        5.00488281e-02, -3.10546875e-01,  9.32617188e-02,  1.21459961e-02,\n",
       "        9.57031250e-02, -1.62109375e-01, -2.33398438e-01, -2.61718750e-01,\n",
       "       -5.12695312e-02, -7.42187500e-02, -4.90722656e-02,  1.06445312e-01,\n",
       "        1.13281250e-01, -2.97851562e-02,  5.46875000e-02,  1.60156250e-01,\n",
       "       -2.12890625e-01, -1.42578125e-01, -1.66015625e-02,  1.54296875e-01,\n",
       "        1.41601562e-02, -9.91210938e-02,  8.44726562e-02,  8.74023438e-02,\n",
       "        2.00195312e-01, -5.07812500e-02,  2.07031250e-01,  1.27929688e-01,\n",
       "        1.75781250e-01,  1.78710938e-01, -1.58691406e-02,  4.05883789e-03,\n",
       "       -5.40161133e-03, -8.48388672e-03, -1.23596191e-03, -1.42578125e-01,\n",
       "       -9.81445312e-02,  3.90625000e-02,  1.22680664e-02,  6.33239746e-04,\n",
       "        5.27343750e-02,  9.37500000e-02, -3.22265625e-02, -2.89062500e-01,\n",
       "        1.27929688e-01, -5.98144531e-03,  7.61718750e-02,  7.17773438e-02,\n",
       "        8.34960938e-02,  2.07519531e-02, -1.96289062e-01,  6.78710938e-02,\n",
       "        2.08007812e-01, -1.08642578e-02, -4.44335938e-02, -2.77343750e-01,\n",
       "       -1.17675781e-01, -1.33789062e-01,  8.15429688e-02,  1.25122070e-02,\n",
       "        1.81640625e-01, -7.51953125e-02,  1.12792969e-01,  7.56835938e-02,\n",
       "        5.05371094e-02, -5.85937500e-02,  5.29785156e-02, -4.37011719e-02,\n",
       "       -5.88378906e-02,  1.54296875e-01,  9.46044922e-03,  1.16210938e-01,\n",
       "       -1.51367188e-01,  1.11083984e-02,  1.41601562e-01, -5.54199219e-02,\n",
       "        1.83593750e-01,  1.17187500e-01, -1.27929688e-01, -7.08007812e-02,\n",
       "        3.72314453e-03, -4.19921875e-02, -9.13085938e-02,  1.09375000e-01,\n",
       "        1.27929688e-01,  2.25830078e-02,  1.93359375e-01,  5.46875000e-02,\n",
       "        2.25830078e-02, -3.22265625e-02,  1.25000000e-01, -5.00488281e-02,\n",
       "       -3.34472656e-02, -6.16455078e-03, -3.27148438e-02, -3.06396484e-02,\n",
       "       -1.31835938e-01, -6.73828125e-02, -1.04980469e-01,  1.26953125e-01,\n",
       "        1.79687500e-01,  7.56835938e-02,  1.08398438e-01,  8.93554688e-02,\n",
       "       -1.72119141e-02,  9.08203125e-02,  6.65283203e-03,  2.30468750e-01,\n",
       "        3.32031250e-02,  1.69921875e-01,  1.49414062e-01, -2.08007812e-01,\n",
       "       -5.98144531e-02, -1.65039062e-01, -9.17968750e-02, -1.46865845e-04,\n",
       "        8.20312500e-02, -1.25000000e-01,  9.88769531e-03,  1.07421875e-01,\n",
       "       -1.04370117e-02, -1.96533203e-02, -2.30712891e-02,  8.88671875e-02,\n",
       "       -1.48925781e-02,  2.18750000e-01,  4.12597656e-02,  2.98828125e-01,\n",
       "       -2.32421875e-01,  1.65039062e-01, -2.23388672e-02, -6.44531250e-02,\n",
       "        1.59179688e-01, -1.70898438e-01,  1.96289062e-01, -9.96093750e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the w2c model\n",
    "# use this or below one\n",
    "w2c = gensim.models.KeyedVectors.load_word2vec_format('../../../temp/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "w2c['go']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list(n):\n",
    "    import random\n",
    "    randomlist = []\n",
    "    for i in range(0,n):\n",
    "        n = random.uniform(-2,2)\n",
    "        randomlist.append(n)\n",
    "    return randomlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_number(num):\n",
    "    mylist = []\n",
    "    for i in range(1,num):\n",
    "        mylist.append(str(i))\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.9955629698771151, -0.059465496674222074, -1.3772573929282714, -0.9162817377290904, -0.5864074138268309, -1.12098597283791, -1.2964507428226644, -1.3732430778289912, 1.7004484486710552, 0.8349857660118722, -1.3973533236125109, 0.6151574203525043, -1.259377136370777, 0.5291990060541827, 0.010753485961588538, 1.6160342961287024, -0.4785420144794177, -0.2870644099209043, 1.745227265232122, -1.3230691306694249, 1.7002480931522963, -1.8504092756603416, 0.6915806789962904, -0.15741799440991988, 1.3166136825602153, -1.1252358108935732, 1.331560599641505, -0.14016120329955806, 0.169990806062982, 0.7653422985663658, -0.9844677153823937, 0.9378378044277453, -0.34353792052422616, 0.6325179713889857, -0.184895178231403, 0.04733398706050407, 0.8409083473158123, 0.7983340560685783, -1.4593710156864188, 0.7819109577438361, -0.28920044958976643, 1.8792492543951322, 1.4950140703224748, 1.7315471773943782, 0.24437031454187297, 0.3363784595429, -0.6742082507145746, 0.9422865473545983, -1.9873643809929642, 1.603496864310491, -0.9256651552451927, 0.3653674739184489, 0.3415409561573921, 0.4261783550819094, 0.3892065512724878, -1.4400636010404626, -1.8312531436159412, -1.582326796056671, -0.22496217363302984, 1.1479585262708736, 1.3062474200641279, -1.2805945737026496, 1.1776276780044945, 1.8487123611518008, 0.5389192406475005, -0.8769245383858704, -1.9046218255366663, 1.0697087878494256, 0.7962055025087649, 1.956827761595842, -1.7453537994722277, -0.3076612462803321, 0.9349540868660062, 1.0543374544711162, 0.9952930436476564, -1.6858675700300125, 1.194805702991609, -1.6832977904294069, 0.3634659720490081, -0.2408986071851409, 0.12443085517557329, -1.9804222596193926, -0.1809420807530171, 0.025847190078671645, -0.541529089912594, -0.9927573097567821, 0.053323757905329394, 0.788922012397868, 1.2768894064710485, 1.2499798791580163, 1.5032920148862048, -1.3280512079736089, -1.2655303302337564, -0.6831338071468855, 1.2057426930058028, 0.9659153056502809, 1.5565851266593929, -0.908237914683919, 1.4576226380098807, -0.400681246150953, 1.4710517018648357, 0.43612070767343614, -1.2982396016992315, 1.4746269982549811, -1.5546125742797412, 1.1162521064088113, -1.430257918664017, -0.08110002981448616, -0.6658434698621325, 1.3014157759850415, 0.8875517799747796, 1.972318102613082, 1.2787441435560298, 0.6011424840226178, -1.5297548229726523, 1.8458569103035205, -1.7733212832730993, 1.1893847479821078, -0.7250750049857637, 1.0252878939676946, -1.4332189869037508, 1.6660372921120326, -1.70262955060233, -0.1028332720358982, 0.18904061244984494, 1.8454342540640858, 0.2868464795916883, 1.5844681531968376, 1.000746539096725, 1.6102391812280246, -1.8746255971347665, -1.6004733163435887, 0.7330677789622277, -1.2539995594161168, -0.1172984952400773, 0.8589055256886398, 0.3479132908210656, 0.05638452289227569, 1.7877897410959251, 1.5628595261275282, 0.505127122031785, -1.1979888762878756, 1.0411545003067255, 0.4661664595935733, 1.2322022470915326, 1.5511674183726707, -1.4867742851602102, -1.072266627642367, 0.8782088103051557, 0.8930713426761487, -0.7947934089285922, -1.387972221075365, -0.871048280847758, 1.7480468085009448, 1.1163082435381657, -1.8733526283963262, 0.4267808487688578, 1.2446280314652425, 1.781169293332744, -0.3450514765928876, 0.1998234151735745, -0.45560812964382125, -0.33468195437213577, 1.0822624835109989, -0.4331441493672292, 1.2808993123393395, 0.5411227393955058, -0.6977244553664921, 0.5974063337115534, 1.0135713517908553, -1.0255708061478566, 1.821651782173849, 0.14235167548816152, -1.7349482056924397, 1.8844958240577068, -1.6052979727664636, 0.7067301261195071, -0.9024088944969311, -0.3386349576272347, 0.33966786401365123, -1.0624158425954295, 0.0340453768405613, -0.4758023926923278, 1.4233615141518552, -0.13969406678441265, -0.45850167045071233, -1.1867900879828386, 0.9276397192967663, 1.9257376590736195, -0.45956562731423123, 1.355762152425204, 0.1029640676197019, -0.2236165107421786, 1.837631087904275, -0.6328801417622052, -1.0303851278554887, 1.0641069798545457, -0.3262528315295201, 0.017593018714274322, -1.4373007640883544, -0.6193695994871597, 1.530941572500908, 1.8203552583216949, 0.14224091284192086, 1.1969856706734796, -1.4355981362693746, 0.9095123799156544, -0.8396106924207891, -1.746276726054825, 0.4905673304064915, -1.9889388018101664, -0.47523689019137016, -1.3094386406481973, 1.6539817162288082, 1.3644635139570638, -1.7984562379615934, 1.583265607550754, -0.6496566317677037, -1.7152846728222118, -0.8775374988412095, 1.5393821432090773, 1.462362072718419, 1.2657475064187773, 1.212288379413145, -1.7698608966520832, 0.21954739179134997, -0.45990747764771367, -0.7731528341797125, -1.7415393408074302, -1.2942131572722486, 1.4229090120854888, -1.3479284543405954, 0.8463597348719287, 0.3120081382688129, 1.4634605902991313, -0.6452092942494922, -0.4401499745462929, -0.952012262857485, 1.0445597564747788, 1.6883433928263445, 0.34590551389749935, 0.9378046058822513, -1.0633784012722405, 1.117915638597383, 0.47968371761116657, 0.49776824947112086, 1.845863523780642, 1.7706783133893929, -1.7478056739310954, 0.6820965902404184, 1.8961723430911466, -0.39720786634517635, -1.889183109696086, 1.4227922516363751, -0.5418910783743653, -1.3595133586174595, 1.2547265667685168, -0.7718896155311143, -0.23843073787689795, 1.0148381931550685, -1.4284813837408539, 0.1998347050278868, -0.18961632743492318, 0.20077286598688238, -0.4099867471282126, -1.7920842666553907, -0.7604053187104993, 0.008591953278029418, 0.13598337430631968, 1.1280034345444605, 0.5417023602188973, -1.0108322117975752, -0.562337090766587, 0.28203492008902664, 1.41186131800489, -0.3750860869972543, -1.49819042747187, -1.7858991811423763, 1.9765204466330832, -1.6256389953462143, 1.0838426469810996, -0.46181805851965807, -0.5238998220496689, 1.4188904836114555, -0.5930818450477147, -1.0267713805328613, -0.8575998024750882, -0.8661488101964196, -1.736557171266611, 1.4329224489169996, 0.47704661798034476, 1.87463268551899, 1.5954103192525184, 0.5539421079198092, -1.8836284372107914, -0.8890034809528902, 0.27731942027312106, 1.6024647009287776, -1.8948081357992845, 1.5376760289978542]\n"
     ]
    }
   ],
   "source": [
    "new_df = pd.DataFrame()\n",
    "final_df = []\n",
    "missed = []\n",
    "l = []\n",
    "for index, row in df.iterrows():\n",
    "    string1 = row['text']\n",
    "    #l.append(string1)\n",
    "    try:\n",
    "        result = document_vector(w2c, string1)\n",
    "        #print(result)\n",
    "        l.append(list(result))\n",
    "        #print(l)\n",
    "        #final_df.append(l)\n",
    "        #a_series = pd.Series(l)\n",
    "        #print(l)\n",
    "\n",
    "        #new_df = new_df.append(a_series,ignore_index=True)\n",
    "    except Exception as e:\n",
    "        list1 = [\n",
    "            0.69671315,\n",
    "            0.049782764,\n",
    "            -0.24523668,\n",
    "            -0.15872465,\n",
    "            -0.0665417,\n",
    "            0.20241983,\n",
    "            0.077576466,\n",
    "            1.9189811,\n",
    "            -0.006817713,\n",
    "            0.06951927,\n",
    "            -0.4152605,\n",
    "            -0.89838743,\n",
    "            -3.7327635,\n",
    "            -0.049629666,\n",
    "            -0.7617386,\n",
    "            -0.5561758,\n",
    "            -0.9451503,\n",
    "            0.035365578,\n",
    "            -1.0393411,\n",
    "            0.2922259,\n",
    "            -0.16664228,\n",
    "            -0.46666014,\n",
    "            0.35039356,\n",
    "            0.40681368,\n",
    "            0.38142973,\n",
    "            ]\n",
    "        l.append(create_list(300))\n",
    "        #missed.append(str(index) + ' ')\n",
    "        #print(e)\n",
    "        #final_df.append(l)\n",
    "        #a_series = pd.Series(l)\n",
    "        #new_df.append(a_series,ignore_index=True)\n",
    "print(l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.21765376 -0.93583841  1.04988693 ... -1.8940708   0.51550383\n",
      "  -1.55306646]\n",
      " [-1.99556297 -0.0594655  -1.37725739 ...  1.6024647  -1.89480814\n",
      "   1.53767603]\n",
      " [ 0.13256966 -0.6736362  -1.75572397 ... -0.94742344  1.93814372\n",
      "   1.46186149]\n",
      " ...\n",
      " [ 0.88892467  1.79208064  0.12857314 ...  1.16218543  0.02861918\n",
      "   0.02193655]\n",
      " [-1.41602923 -0.53813091  1.53977083 ...  0.73687115  0.91999201\n",
      "   0.94353719]\n",
      " [-0.72580542  0.75384585 -0.76340768 ...  0.48940175  0.91176122\n",
      "   1.67633513]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "l = np.array(l)\n",
    "print(l)\n",
    "print(type(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.78234624,  9.06416159, 11.04988693, ...,  8.1059292 ,\n",
       "        10.51550383,  8.44693354],\n",
       "       [ 8.00443703,  9.9405345 ,  8.62274261, ..., 11.6024647 ,\n",
       "         8.10519186, 11.53767603],\n",
       "       [10.13256966,  9.3263638 ,  8.24427603, ...,  9.05257656,\n",
       "        11.93814372, 11.46186149],\n",
       "       ...,\n",
       "       [10.88892467, 11.79208064, 10.12857314, ..., 11.16218543,\n",
       "        10.02861918, 10.02193655],\n",
       "       [ 8.58397077,  9.46186909, 11.53977083, ..., 10.73687115,\n",
       "        10.91999201, 10.94353719],\n",
       "       [ 9.27419458, 10.75384585,  9.23659232, ..., 10.48940175,\n",
       "        10.91176122, 11.67633513]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a function that adds 10\n",
    "add_10 = lambda i: i + 10\n",
    "\n",
    "# Create a vectorized function\n",
    "vectorized_add_10 = np.vectorize(add_10)\n",
    "\n",
    "# Apply function to all elements in matrix\n",
    "new_l = vectorized_add_10(l)\n",
    "new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Just happened a terrible car crash',\n",
       "       'Heard about #earthquake is different cities, stay safe everyone.',\n",
       "       'there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all',\n",
       "       ..., 'Green Line derailment in Chicago http://t.co/UtbXLcBIuY',\n",
       "       'MEG issues Hazardous Weather Outlook (HWO) http://t.co/3X6RBQJHn3',\n",
       "       '#CityofCalgary has activated its Municipal Emergency Plan. #yycstorm'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../dataset/test.csv')\n",
    "x_test = test.text.values\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "7613\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "labels = df.target.values\n",
    "print(labels)\n",
    "print(len(labels))\n",
    "print(type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.11734658 10.1181292   9.33978054 ...  9.77718602 11.30327314\n",
      "   9.9503195 ]\n",
      " [10.5517871  11.73065543  8.49385809 ... 10.37831197  8.35645726\n",
      "  11.48569914]\n",
      " [10.17318796  9.85098199  9.43240414 ... 11.57884388  9.28201197\n",
      "   8.65954162]\n",
      " ...\n",
      " [10.19493256 11.08589722  9.70139935 ... 10.30027151  8.73070422\n",
      "   8.75654313]\n",
      " [ 9.60852369  8.58445431 10.67662507 ... 10.28894443  8.80182757\n",
      "  10.88313309]\n",
      " [ 8.04942151 11.74063548 10.04462099 ...  8.0370108  10.30192352\n",
      "   8.76515417]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras.layers import Input,Dense,Embedding,LSTM,Dropout,Activation\n",
    "from keras.layers import Bidirectional,GlobalMaxPool1D\n",
    "# Split the data to train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_l,labels, test_size=0.2)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_size = 300\n",
    "max_features = 10000\n",
    "maxlen = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 300, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 300, 128)          186880    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,188,961\n",
      "Trainable params: 3,188,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "inp = Input(shape = (maxlen,))\n",
    "x = Embedding(max_features,embedded_size)(inp)\n",
    "x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16,activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1,activation = 'sigmoid')(x)\n",
    "model = Model(inputs = inp,outputs = x)\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6090/6090 [==============================] - 42s 7ms/step - loss: 0.6889 - accuracy: 0.5499\n",
      "Epoch 2/10\n",
      "6090/6090 [==============================] - 23s 4ms/step - loss: 0.6844 - accuracy: 0.5716\n",
      "Epoch 3/10\n",
      "6090/6090 [==============================] - 22s 4ms/step - loss: 0.6823 - accuracy: 0.5716\n",
      "Epoch 4/10\n",
      "6090/6090 [==============================] - 22s 4ms/step - loss: 0.6832 - accuracy: 0.5716\n",
      "Epoch 5/10\n",
      "6090/6090 [==============================] - 22s 4ms/step - loss: 0.6829 - accuracy: 0.5716\n",
      "Epoch 6/10\n",
      "6090/6090 [==============================] - 22s 4ms/step - loss: 0.6838 - accuracy: 0.5716\n",
      "Epoch 7/10\n",
      "6090/6090 [==============================] - 24s 4ms/step - loss: 0.6841 - accuracy: 0.5716\n",
      "Epoch 8/10\n",
      "6090/6090 [==============================] - 24s 4ms/step - loss: 0.6824 - accuracy: 0.5716\n",
      "Epoch 9/10\n",
      "6090/6090 [==============================] - 25s 4ms/step - loss: 0.6829 - accuracy: 0.5716\n",
      "Epoch 10/10\n",
      "6090/6090 [==============================] - 24s 4ms/step - loss: 0.6834 - accuracy: 0.5716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7faadaccec50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "model.fit(X_train, y_train, batch_size=512, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "mlvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
