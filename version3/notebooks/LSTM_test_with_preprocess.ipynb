{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kiranbal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "nltk.download('stopwords')\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "def removeUnicode(text):\n",
    "\t#Removes unicode strings like \"\\u002c\" and \"x96\"\n",
    "\ttext = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n",
    "\ttext = re.sub(r'[^\\x00-\\x7f]',r'',text)\n",
    "\treturn text\n",
    "\n",
    "def replaceURL(text):\n",
    "\t#Replaces url address with \"url\" \n",
    "\ttext = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
    "\ttext = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\treturn text\n",
    "\n",
    "def replaceAtUser(text):\n",
    "\t#Replaces \"@user\" with \"atUser\"\n",
    "\ttext = re.sub('@[^\\s]+',' ',text)\n",
    "\treturn text\n",
    "\n",
    "def removeHashtag(text):\n",
    "\t#Removes hastag in front of a word\n",
    "\ttext = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\treturn text\n",
    "\n",
    "def removeNumbers(text):\n",
    "\t#Removes integers\n",
    "\ttext = ''.join([i for i in text if not i.isdigit()])         \n",
    "\treturn text\n",
    "\n",
    "def replaceMulExcl(text):\n",
    "\t#Replaces repetitions of exlamation marks\n",
    "\ttext = re.sub(r\"(\\!)\\1+\", '!', text)\n",
    "\treturn text\n",
    "\n",
    "def replaceMulQues(text):\n",
    "\t#Replaces repetitions of question marks\n",
    "\ttext = re.sub(r\"(\\?)\\1+\", '?', text)\n",
    "\treturn text\n",
    "\n",
    "def replaceMulStop(text):\n",
    "\t#Replaces repetitions of stop marks\n",
    "\ttext = re.sub(r\"(\\.)\\1+\", '.', text)\n",
    "\treturn text\n",
    "\n",
    "def countMulExcl(text):\n",
    "\t#count repetitions of exlamation marks\n",
    "\treturn len(re.findall(r\"(\\!)\\1+\", text))\n",
    "\n",
    "def countMulQues(text):\n",
    "\t#Count repetitions of question marks\n",
    "\treturn len(re.findall(r\"(\\?)\\1+\", text))\n",
    "\n",
    "def countMulStop(text):\n",
    "\t#Count repetitions of stop marks\n",
    "\treturn len(re.findall(r\"(\\.)\\1+\", text))\n",
    "\n",
    "def countElongated(text):\n",
    "\t#count of how many words are elongated\n",
    "\tregex = re.compile(r\"(.)\\1{2}\")\n",
    "\treturn len([word for word in text.split() if regex.search(word)])\n",
    "\n",
    "def countAllCaps(text):\n",
    "\t#count of how many words are all caps\n",
    "\treturn len(re.findall(\"[A-Z0-9]{3,}\", text))\n",
    "\n",
    "#Creates a dictionary with slangs and their equivalents and replaces them\n",
    "with open('./temp_files/slang.txt') as file:\n",
    "\tslang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "\tfor line in file if line.strip())\n",
    "\n",
    "slang_words = sorted(slang_map, key=len, reverse=True)\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
    "\n",
    "#punctuation list for replacing\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '|', ';', \"'\", '$', '&', '/', '[', ']', '%', '=', '*', '+', '\\\\', '•',  '~', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def removePuncts(x):\n",
    "\tx = str(x)\n",
    "\tfor punct in puncts:\n",
    "\t\tif punct in x:\n",
    "\t\t\tx = x.replace(punct, f' ')\n",
    "\treturn x\n",
    "\n",
    "\n",
    "\n",
    "def countSlang(text):\n",
    "\t# counts how many slang words and a list of found slangs\n",
    "\tslangCounter = 0\n",
    "\tslangsFound = []\n",
    "\ttokens = nltk.word_tokenize(text)\n",
    "\tfor word in tokens:\n",
    "\t\tif word in slang_words:\n",
    "\t\t\tslangsFound.append(word)\n",
    "\t\t\tslangCounter += 1\n",
    "\treturn slangCounter, slangsFound\n",
    "\n",
    "#Replaces contractions from a string to their equivalents\n",
    "contraction_patterns = [ (r'I\\'m', 'I am'),(r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "\t\t\t\t\t\t (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "def replaceContraction(text):\n",
    "\tpatterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "\tfor (pattern, repl) in patterns:\n",
    "\t\t(text, count) = re.subn(pattern, repl, text)\n",
    "\treturn text\n",
    "\n",
    "def replaceElongated(word):\n",
    "\t#Replaces an elongated word with its basic form\n",
    "\n",
    "\trepeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "\trepl = r'\\1\\2\\3'\n",
    "\tif wordnet.synsets(word):\n",
    "\t\treturn word\n",
    "\trepl_word = repeat_regexp.sub(repl, word)\n",
    "\tif repl_word != word:      \n",
    "\t\treturn replaceElongated(repl_word)\n",
    "\telse:       \n",
    "\t\treturn repl_word\n",
    "\n",
    "def removeEmoticons(text):\n",
    "\t#Removes emoticons from text \n",
    "\ttext = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n",
    "\treturn text\n",
    "\n",
    "def countEmoticons(text):\n",
    "\t#Input: a text, Output: how many emoticons\n",
    "\treturn len(re.findall(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', text))\n",
    "\n",
    "\n",
    "### Spell Correction begin ###\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('./temp_files/spell_correction.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "\t#P robability of `word`.\n",
    "\treturn WORDS[word] / N\n",
    "\n",
    "def spellCorrection(word): \n",
    "\t#Most probable spelling correction for word.\n",
    "\treturn max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "\t#Generate possible spelling corrections for word.\n",
    "\treturn (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "\t#The subset of `words` that appear in the dictionary of WORDS.\n",
    "\treturn set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "\t#All edits that are one edit away from `word`.\n",
    "\tletters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\tsplits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "\tdeletes    = [L + R[1:]               for L, R in splits if R]\n",
    "\ttransposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "\treplaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "\tinserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "\treturn set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "\t#All edits that are two edits away from `word`.\n",
    "\treturn (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "### Spell Correction End ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from time import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "def k_prep(inputText = 'null'):\n",
    "\t'''\n",
    "\tc = input(\"\\n1.Enter or 2.default or 3.df ? :\")\n",
    "\n",
    "\tif c == \"2\":\n",
    "\t\ttext = \"AFRICA,#AFRICANBAZE: asap goaaaaal @Breaking !!!! news:Nigeria oooooooh :-D aren't flag???? wont set ablaze..... 12000 in America. http://t.co/2nndBGwyEi,1\"\n",
    "\t\t#print(text)\n",
    "\t\n",
    "\telif c == \"3\" :\n",
    "\t\ttext = inputText\n",
    "\n",
    "\telse:\n",
    "\t\ttext = input(\"\\nEnter the tweet: \")\n",
    "\t'''\n",
    "\n",
    "\ttext = inputText\n",
    "\t#print(\"\\nReplacing url from tweet\\n\")\n",
    "\ttext = replaceURL(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nReplacing atuser from tweet\\n\")\n",
    "\ttext = replaceAtUser(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nremoving hashtag in tweet\\n\")\n",
    "\ttext = removeHashtag(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nreplace at user in tweet\\n\")\n",
    "\ttext = replaceAtUser(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t'''\n",
    "\t#print(\"\\nremoving stopwords\\n\")\n",
    "\t#nltk.download('stopwords')\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tstop = set(stopwords.words('english'))\n",
    "\n",
    "\td=[]\n",
    "\td.append([x for x in text.split() if x not in stop])\n",
    "\td = d[0]\n",
    "\ttext = ' '.join(d)\n",
    "\t#print(text)\n",
    "\t'''\n",
    "\n",
    "\t#print(\"\\nremove numbers from tweet\\n\")\n",
    "\ttext = removeNumbers(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nremove emoticons from tweet\\n\")\n",
    "\ttext = removeEmoticons(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#couting multple punctuations\n",
    "\t#print(\"\\ncounting multiple punctuations\\n\")\n",
    "\tMultiExclMarks = 0\n",
    "\tMultiQuesMarks = 0\n",
    "\tMultiStopMarks = 0\n",
    "\n",
    "\tMultiExclMarks += countMulExcl(text)\n",
    "\tMultiQuesMarks += countMulQues(text)\n",
    "\tMultiStopMarks += countMulStop(text)\n",
    "\n",
    "\t#print(MultiExclMarks,MultiQuesMarks,MultiStopMarks)\n",
    "\n",
    "\t#print(\"\\nremove multiexclamations from tweet\\n\")\n",
    "\ttext = replaceMulExcl(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nremove multiquestionmarks from tweet\\n\")\n",
    "\ttext = replaceMulQues(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nremove multistopmarks from tweet\\n\")\n",
    "\ttext = replaceMulStop(text)\n",
    "\t#print(text)\n",
    "\n",
    "\n",
    "\t#print(\"\\nshortening elongated words\\n\")\n",
    "\ttotalElongated = 0\n",
    "\ttotalElongated += countElongated(text)\n",
    "\t#print(totalElongated)\n",
    "\n",
    "\tregex1 = re.compile(r\"(.)\\1{2}\")\n",
    "\tl=[]\n",
    "\tfor word in text.split():\n",
    "\t\tif(regex1.search(word)):\n",
    "\t\t\tnew_word = replaceElongated(word)\n",
    "\t\t\t##print(new_word)\n",
    "\t\t\tl.append(new_word)\n",
    "\t\telse:\n",
    "\t\t\tl.append(word)\n",
    "\ttext = ' '.join(l)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nRemoving punctuations except ?!\\n\")\n",
    "\ttext = removePuncts(text)\n",
    "\t#print(text)\n",
    "\t\n",
    "\t#print(\"\\nexpanding slangs in tweet\\n\")\n",
    "\ttext = replaceSlang(text)\n",
    "\t#print(text)\n",
    "\n",
    "\t#print(\"\\nreplace contractions in tweet\\n\")\n",
    "\ttext = replaceContraction(text)\n",
    "\t#print(text)\n",
    "\t\n",
    "\t#print(\\nTokenizing the text\\n\")\n",
    "\ttext = word_tokenize(text)\n",
    "\t\n",
    "\t#print(\"\\nLemmatizing the text\\n\")\n",
    "\tlemma = WordNetLemmatizer()\n",
    "\t\n",
    "\tlist1 = []\n",
    "\tfor txt in text:\n",
    "\t\tlist1.append(lemma.lemmatize(txt))\n",
    "\t\t\n",
    "\treturn list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kiranbal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/kiranbal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id keyword location  \\\n",
      "0         1     NaN      NaN   \n",
      "1         4     NaN      NaN   \n",
      "2         5     NaN      NaN   \n",
      "3         6     NaN      NaN   \n",
      "4         7     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "7608  10869     NaN      NaN   \n",
      "7609  10870     NaN      NaN   \n",
      "7610  10871     NaN      NaN   \n",
      "7611  10872     NaN      NaN   \n",
      "7612  10873     NaN      NaN   \n",
      "\n",
      "                                                   text  target  \n",
      "0     Our Deeds are the Reason of this earthquake Ma...       1  \n",
      "1                 Forest fire near La Ronge Sask Canada       1  \n",
      "2     All resident asked to shelter in place are bei...       1  \n",
      "3     people receive wildfire evacuation order in Ca...       1  \n",
      "4     Just got sent this photo from Ruby Alaska a sm...       1  \n",
      "...                                                 ...     ...  \n",
      "7608  Two giant crane holding a bridge collapse into...       1  \n",
      "7609  The out of control wild fire in California eve...       1  \n",
      "7610                     M UTC ? km S of Volcano Hawaii       1  \n",
      "7611  Police investigating after an e bike collided ...       1  \n",
      "7612  The Latest More Homes Razed by Northern Califo...       1  \n",
      "\n",
      "[7613 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "df = pd.read_csv(\"../dataset/train.csv\")\n",
    "df['text'] = df['text'].apply(k_prep)\n",
    "\n",
    "def joinData(list1):\n",
    "  new_data = \" \".join(list1)\n",
    "  return new_data\n",
    "\n",
    "df['text'] = df['text'].apply(joinData)\n",
    "df.to_csv('df_train_preprocessed.csv',index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>All resident asked to shelter in place are bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>people receive wildfire evacuation order in Ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska a sm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target\n",
       "0   1  Our Deeds are the Reason of this earthquake Ma...       1\n",
       "1   4              Forest fire near La Ronge Sask Canada       1\n",
       "2   5  All resident asked to shelter in place are bei...       1\n",
       "3   6  people receive wildfire evacuation order in Ca...       1\n",
       "4   7  Just got sent this photo from Ruby Alaska a sm...       1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df = pd.read_csv('./df_train_preprocessed.csv',usecols=['id','text','target'])\n",
    "#fixing conflicts\n",
    "target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "input_df.at[input_df['id'].isin(target_error),'target'] = 0\n",
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras.layers import Input,Dense,Embedding,LSTM,Dropout,Activation\n",
    "from keras.layers import Bidirectional,GlobalMaxPool1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Our Deeds are the Reason of this earthquake May ALLAH Forgive u all',\n",
       "       'Forest fire near La Ronge Sask Canada',\n",
       "       'All resident asked to shelter in place are being notified by officer No other evacuation or shelter in place order are expected',\n",
       "       ..., 'M UTC ? km S of Volcano Hawaii',\n",
       "       'Police investigating after an e bike collided with a car in Little Portugal E bike rider suffered serious non life threatening injury',\n",
       "       'The Latest More Homes Razed by Northern California Wildfire ABC News'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_size = 100\n",
    "max_features = 10000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../dataset/test.csv')\n",
    "test_X = test.text.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df ,val_df = train_test_split(input_df,test_size = 0.1 , random_state = 43)\n",
    "train_X = train_df.text.values\n",
    "val_X = val_df.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the text data using Tokenizer function\n",
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "\n",
    "with open(\"tokenizer_backup\",'wb') as file:\n",
    "    pickle.dump(tokenizer,file)\n",
    "\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the values\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df.target.values\n",
    "val_y = val_df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Building layers of embedding\n",
    "model = tf.keras.Sequential([\n",
    "     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length), \n",
    "     tf.keras.layers.Flatten(),\n",
    "     tf.keras.layers.Dense(6, activation='relu'),\n",
    "     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kiranbal/anaconda3/envs/mlvenv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/kiranbal/anaconda3/envs/mlvenv/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 128)          84480     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,086,561\n",
      "Trainable params: 1,086,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "inp = Input(shape = (maxlen,))\n",
    "x = Embedding(max_features,embedded_size)(inp)\n",
    "x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16,activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1,activation = 'sigmoid')(x)\n",
    "model = Model(inputs = inp,outputs = x)\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./temp_files/best_model2_16bz_preprocessed.h5', monitor='val_accuracy', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/10\n",
      "6851/6851 [==============================] - 5s 774us/step - loss: 0.3390 - accuracy: 0.8809 - val_loss: 0.3890 - val_accuracy: 0.8346\n",
      "Epoch 2/10\n",
      "6851/6851 [==============================] - 5s 768us/step - loss: 0.2952 - accuracy: 0.8879 - val_loss: 0.4105 - val_accuracy: 0.8373\n",
      "Epoch 3/10\n",
      "6851/6851 [==============================] - 5s 762us/step - loss: 0.2697 - accuracy: 0.8994 - val_loss: 0.4103 - val_accuracy: 0.8360\n",
      "Epoch 4/10\n",
      "6851/6851 [==============================] - 5s 770us/step - loss: 0.2490 - accuracy: 0.9102 - val_loss: 0.4124 - val_accuracy: 0.8320\n",
      "Epoch 5/10\n",
      "6851/6851 [==============================] - 5s 770us/step - loss: 0.2304 - accuracy: 0.9209 - val_loss: 0.4244 - val_accuracy: 0.8333\n",
      "Epoch 6/10\n",
      "6851/6851 [==============================] - 5s 772us/step - loss: 0.2142 - accuracy: 0.9275 - val_loss: 0.4395 - val_accuracy: 0.8346\n",
      "Epoch 7/10\n",
      "6851/6851 [==============================] - 5s 766us/step - loss: 0.2021 - accuracy: 0.9342 - val_loss: 0.4510 - val_accuracy: 0.8294\n",
      "Epoch 8/10\n",
      "6851/6851 [==============================] - 5s 768us/step - loss: 0.1862 - accuracy: 0.9387 - val_loss: 0.4619 - val_accuracy: 0.8320\n",
      "Epoch 9/10\n",
      "6851/6851 [==============================] - 5s 785us/step - loss: 0.1755 - accuracy: 0.9437 - val_loss: 0.4751 - val_accuracy: 0.8255\n",
      "Epoch 10/10\n",
      "6851/6851 [==============================] - 5s 771us/step - loss: 0.1643 - accuracy: 0.9457 - val_loss: 0.4917 - val_accuracy: 0.8241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fbf5fbf0790>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "model.fit(train_X, train_y,callbacks=[checkpoint], batch_size=512, epochs=10, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = model.predict([test_X],batch_size = 1024,verbose = 1)\n",
    "#preds\n",
    "import pickle\n",
    "with open(\"lstm_model_backup\",'wb') as file:\n",
    "    pickle.dump(model,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6122646 ],\n",
       "       [0.57825834],\n",
       "       [0.8766701 ],\n",
       "       ...,\n",
       "       [0.82612413],\n",
       "       [0.76766604],\n",
       "       [0.55688345]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./temp_files/best_model2_16bz_preprocessed.h5')\n",
    "#test_pred = model.predict(test_input)\n",
    "probabilities = model.predict(test_X)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = (probabilities > 0.5).astype(int)\n",
    "predictions = np.ndarray.flatten(predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id keyword location  \\\n",
      "0         0     NaN      NaN   \n",
      "1         2     NaN      NaN   \n",
      "2         3     NaN      NaN   \n",
      "3         9     NaN      NaN   \n",
      "4        11     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "3258  10861     NaN      NaN   \n",
      "3259  10865     NaN      NaN   \n",
      "3260  10868     NaN      NaN   \n",
      "3261  10874     NaN      NaN   \n",
      "3262  10875     NaN      NaN   \n",
      "\n",
      "                                                   text  \n",
      "0                    Just happened a terrible car crash  \n",
      "1     Heard about #earthquake is different cities, s...  \n",
      "2     there is a forest fire at spot pond, geese are...  \n",
      "3              Apocalypse lighting. #Spokane #wildfires  \n",
      "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
      "...                                                 ...  \n",
      "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
      "3259  Storm in RI worse than last hurricane. My city...  \n",
      "3260  Green Line derailment in Chicago http://t.co/U...  \n",
      "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
      "3262  #CityofCalgary has activated its Municipal Eme...  \n",
      "\n",
      "[3263 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "original_test_df = pd.read_csv(\"../dataset/test.csv\")\n",
    "print(original_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      Just happened a terrible car crash\n",
      "1       Heard about earthquake is different city stay ...\n",
      "2       there is a forest fire at spot pond goose are ...\n",
      "3                    Apocalypse lighting Spokane wildfire\n",
      "4               Typhoon Soudelor kill in China and Taiwan\n",
      "                              ...                        \n",
      "3258    EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...\n",
      "3259    Storm in RI worse than last hurricane My city ...\n",
      "3260                     Green Line derailment in Chicago\n",
      "3261              MEG issue Hazardous Weather Outlook HWO\n",
      "3262    CityofCalgary ha activated it Municipal Emerge...\n",
      "Name: text, Length: 3263, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#filter test set\n",
    "original_test_df['text'] = original_test_df['text'].apply(k_prep)\n",
    "original_test_df['text'] = original_test_df['text'].apply(joinData)\n",
    "print(original_test_df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>0.612265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about earthquake is different city stay ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.578258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond goose are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting Spokane wildfire</td>\n",
       "      <td>0</td>\n",
       "      <td>0.357839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kill in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.962055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We re shaking It s an earthquake</td>\n",
       "      <td>1</td>\n",
       "      <td>0.555409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>They d probably still show more life than Arse...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.160086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hey ! How are you ?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What a nice hat ?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fuck off !</td>\n",
       "      <td>0</td>\n",
       "      <td>0.329697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>No I don t like cold !</td>\n",
       "      <td>0</td>\n",
       "      <td>0.195232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NO ! Don t do that !</td>\n",
       "      <td>0</td>\n",
       "      <td>0.341771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>No don t tell me that !</td>\n",
       "      <td>0</td>\n",
       "      <td>0.297597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What if ? !</td>\n",
       "      <td>0</td>\n",
       "      <td>0.257607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Awesome !</td>\n",
       "      <td>0</td>\n",
       "      <td>0.284902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Birmingham Wholesale Market is ablaze BBC News...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.730257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>will you wear short for race ablaze ?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.218715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PreviouslyOnDoyinTv Toke MakinwaÛªs marriage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Check these out not safe for work</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PSA IÛªm splitting my personality ? techie fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.235240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>beware world ablaze sierra leone amp guap</td>\n",
       "      <td>0</td>\n",
       "      <td>0.342439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Burning Man Ablaze ! by Turban Diva via</td>\n",
       "      <td>0</td>\n",
       "      <td>0.355130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Not a dis song People will take thing and run ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.210497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Rape victim dy a she set herself ablaze A year...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.499639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SETTING MYSELF ABLAZE</td>\n",
       "      <td>0</td>\n",
       "      <td>0.239896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>the bin in front of the field by my house wer ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nowplaying Alfons Ablaze on Puls Radio pulsradio</td>\n",
       "      <td>0</td>\n",
       "      <td>0.246716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Burning Rahm Let s hope City Hall build a gian...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.328454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hurt but her eye ablaze with insulted anger</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Accident cleared in PaTurnpike on PATP EB betw...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.868248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Just got to love burning your self on a damn c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.242648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I hate badging shit in accident</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Car Recorder ZeroEdgeå Dual lens Car Camera Ve...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.773130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Coincidence Or Curse ? Still Unresolved Secret...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.367802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Accident on A near Lewes is it Kingston Rounda...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.894647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>If you pretend to feel a certain way the feeli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>For Legal and Medical Referral Service Call u ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>There s a construction guy working on the Disn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.351985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>I feel like I m going to do it on accident Tee...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.258342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>On the M northbound between junction J and JA ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.592781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>So if I say that I met her by accident this we...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ACCIDENT HIT AND RUN COLD at BLOCK OF SE VISTA...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.685721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>this happened on accident but I like it</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Please donate and spread the word ! A training...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.453811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Please like and share our new page for our Ind...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>foi no ROH Aftershock Las Vegas procura no pir...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.327298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Schoolboy ÛÒ Aftershock Original Mix Excision...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>IR ICEMOON AFTERSHOCK Dubstep TrapMusic DnB ED...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>When the aftershock happened Nepal we were the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.354207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>IR ICEMOON AFTERSHOCK Dubstep TrapMusic DnB ED...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Stop saying I Wish and start saying I Will ÛÒ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.246115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>I want to go to Aftershock in October because ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.124946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>We are still living in the aftershock of Hiros...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.768236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>IR ICEMOON AFTERSHOCK Dubstep TrapMusic DnB ED...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Aftershock</td>\n",
       "      <td>0</td>\n",
       "      <td>0.265005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>There is no victory at bargain basement price ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.293014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Bo had by far the best competitive map in my o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Brass and Copper in Cataclysm amp AfterShock !</td>\n",
       "      <td>0</td>\n",
       "      <td>0.277331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>You should be happy I don t use Aftershock Tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.075399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Aftershock</td>\n",
       "      <td>0</td>\n",
       "      <td>0.265005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  prediction  \\\n",
       "0                  Just happened a terrible car crash           1   \n",
       "1   Heard about earthquake is different city stay ...           1   \n",
       "2   there is a forest fire at spot pond goose are ...           1   \n",
       "3                Apocalypse lighting Spokane wildfire           0   \n",
       "4           Typhoon Soudelor kill in China and Taiwan           1   \n",
       "5                    We re shaking It s an earthquake           1   \n",
       "6   They d probably still show more life than Arse...           0   \n",
       "7                                 Hey ! How are you ?           0   \n",
       "8                                   What a nice hat ?           0   \n",
       "9                                          Fuck off !           0   \n",
       "10                             No I don t like cold !           0   \n",
       "11                               NO ! Don t do that !           0   \n",
       "12                            No don t tell me that !           0   \n",
       "13                                        What if ? !           0   \n",
       "14                                          Awesome !           0   \n",
       "15  Birmingham Wholesale Market is ablaze BBC News...           1   \n",
       "16              will you wear short for race ablaze ?           0   \n",
       "17  PreviouslyOnDoyinTv Toke MakinwaÛªs marriage ...           1   \n",
       "18                  Check these out not safe for work           0   \n",
       "19  PSA IÛªm splitting my personality ? techie fo...           0   \n",
       "20          beware world ablaze sierra leone amp guap           0   \n",
       "21            Burning Man Ablaze ! by Turban Diva via           0   \n",
       "22  Not a dis song People will take thing and run ...           0   \n",
       "23  Rape victim dy a she set herself ablaze A year...           0   \n",
       "24                              SETTING MYSELF ABLAZE           0   \n",
       "25  the bin in front of the field by my house wer ...           0   \n",
       "26   nowplaying Alfons Ablaze on Puls Radio pulsradio           0   \n",
       "27  Burning Rahm Let s hope City Hall build a gian...           0   \n",
       "28        hurt but her eye ablaze with insulted anger           0   \n",
       "29  Accident cleared in PaTurnpike on PATP EB betw...           1   \n",
       "30  Just got to love burning your self on a damn c...           0   \n",
       "31                    I hate badging shit in accident           1   \n",
       "32  Car Recorder ZeroEdgeå Dual lens Car Camera Ve...           1   \n",
       "33  Coincidence Or Curse ? Still Unresolved Secret...           0   \n",
       "34  Accident on A near Lewes is it Kingston Rounda...           1   \n",
       "35  If you pretend to feel a certain way the feeli...           0   \n",
       "36  For Legal and Medical Referral Service Call u ...           1   \n",
       "37  There s a construction guy working on the Disn...           0   \n",
       "38  I feel like I m going to do it on accident Tee...           0   \n",
       "39  On the M northbound between junction J and JA ...           1   \n",
       "40  So if I say that I met her by accident this we...           0   \n",
       "41  ACCIDENT HIT AND RUN COLD at BLOCK OF SE VISTA...           1   \n",
       "42            this happened on accident but I like it           0   \n",
       "43  Please donate and spread the word ! A training...           0   \n",
       "44  Please like and share our new page for our Ind...           0   \n",
       "45  foi no ROH Aftershock Las Vegas procura no pir...           0   \n",
       "46  Schoolboy ÛÒ Aftershock Original Mix Excision...           0   \n",
       "47  IR ICEMOON AFTERSHOCK Dubstep TrapMusic DnB ED...           0   \n",
       "48  When the aftershock happened Nepal we were the...           0   \n",
       "49  IR ICEMOON AFTERSHOCK Dubstep TrapMusic DnB ED...           0   \n",
       "50  Stop saying I Wish and start saying I Will ÛÒ...           0   \n",
       "51  I want to go to Aftershock in October because ...           0   \n",
       "52  We are still living in the aftershock of Hiros...           1   \n",
       "53  IR ICEMOON AFTERSHOCK Dubstep TrapMusic DnB ED...           0   \n",
       "54                                         Aftershock           0   \n",
       "55  There is no victory at bargain basement price ...           0   \n",
       "56  Bo had by far the best competitive map in my o...           0   \n",
       "57     Brass and Copper in Cataclysm amp AfterShock !           0   \n",
       "58  You should be happy I don t use Aftershock Tha...           0   \n",
       "59                                         Aftershock           0   \n",
       "\n",
       "    probabilities  \n",
       "0        0.612265  \n",
       "1        0.578258  \n",
       "2        0.876670  \n",
       "3        0.357839  \n",
       "4        0.962055  \n",
       "5        0.555409  \n",
       "6        0.160086  \n",
       "7        0.180907  \n",
       "8        0.281300  \n",
       "9        0.329697  \n",
       "10       0.195232  \n",
       "11       0.341771  \n",
       "12       0.297597  \n",
       "13       0.257607  \n",
       "14       0.284902  \n",
       "15       0.730257  \n",
       "16       0.218715  \n",
       "17       0.753921  \n",
       "18       0.193283  \n",
       "19       0.235240  \n",
       "20       0.342439  \n",
       "21       0.355130  \n",
       "22       0.210497  \n",
       "23       0.499639  \n",
       "24       0.239896  \n",
       "25       0.245124  \n",
       "26       0.246716  \n",
       "27       0.328454  \n",
       "28       0.266312  \n",
       "29       0.868248  \n",
       "30       0.242648  \n",
       "31       0.508383  \n",
       "32       0.773130  \n",
       "33       0.367802  \n",
       "34       0.894647  \n",
       "35       0.222524  \n",
       "36       0.875433  \n",
       "37       0.351985  \n",
       "38       0.258342  \n",
       "39       0.592781  \n",
       "40       0.138177  \n",
       "41       0.685721  \n",
       "42       0.372420  \n",
       "43       0.453811  \n",
       "44       0.150535  \n",
       "45       0.327298  \n",
       "46       0.245191  \n",
       "47       0.110743  \n",
       "48       0.354207  \n",
       "49       0.110743  \n",
       "50       0.246115  \n",
       "51       0.124946  \n",
       "52       0.768236  \n",
       "53       0.110743  \n",
       "54       0.265005  \n",
       "55       0.293014  \n",
       "56       0.222952  \n",
       "57       0.277331  \n",
       "58       0.075399  \n",
       "59       0.265005  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = pd.DataFrame({'text' : original_test_df['text'],'prediction' : predictions,'probabilities' : np.ndarray.flatten(probabilities)})\n",
    "#df.to_csv(\"test_df.csv\", index=False)\n",
    "df_final[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictions.tolist()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv(\"../dataset/sample_submission.csv\")\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  target\n",
      "0         0       0\n",
      "1         2       1\n",
      "2         3       1\n",
      "3         9       0\n",
      "4        11       1\n",
      "...     ...     ...\n",
      "3258  10861       1\n",
      "3259  10865       1\n",
      "3260  10868       1\n",
      "3261  10874       1\n",
      "3262  10875       1\n",
      "\n",
      "[3263 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "submit['target'] = result\n",
    "print(submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(\"./temp_files/lstm_submit_callbacks_corrected_16bz.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "mlvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
